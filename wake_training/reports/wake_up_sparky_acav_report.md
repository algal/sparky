# Wake Word Training Report: `wake up sparky` (ACAV100M negatives)

Date: Tue Feb 10, 2026 (PST)

This report summarizes the training outputs and offline evaluations for a single custom wake word model intended to recognize:

- **Target phrase:** `"wake up sparky"`

## Artifacts

- **Exported ONNX model (final):** `sparky_acav_artifacts/wake_up_sparky_acav.onnx`
- **Training config used:** `configs/wake_up_sparky_acav.yaml`
- **Augmented clip feature arrays (generated by `--augment_clips`):**
  - Positives (train): `sparky_acav_artifacts/wake_up_sparky_acav/positive_features_train.npy` (shape `(40000, 16, 96)`)
  - Negatives (train): `sparky_acav_artifacts/wake_up_sparky_acav/negative_features_train.npy` (shape `(40000, 16, 96)`)
  - Positives (test): `sparky_acav_artifacts/wake_up_sparky_acav/positive_features_test.npy` (shape `(8000, 16, 96)`)
  - Negatives (test): `sparky_acav_artifacts/wake_up_sparky_acav/negative_features_test.npy` (shape `(8000, 16, 96)`)

## What “accuracy / recall / precision” mean here

The training script reports metrics on a **synthetic held-out test set** of generated/augmented wakeword clips (positives) and adversarial/augmented non-wakeword clips (negatives), using a **binary threshold of 0.5** on the classifier output score.

With:

- TP = true positives (positive clips with score ≥ threshold)
- FN = false negatives (positive clips with score < threshold)
- FP = false positives (negative clips with score ≥ threshold)
- TN = true negatives (negative clips with score < threshold)

Then:

- **Accuracy** = (TP + TN) / (TP + TN + FP + FN)
- **Recall** = TP / (TP + FN)
- **Precision** = TP / (TP + FP)

Note: these metrics are **not** the same as “false triggers per hour” in a streaming, always-on system. FP/hour is evaluated on a separate background feature stream.

## Training summary (as printed by `openwakeword/train.py`)

The training run printed:

- Final Model Accuracy: `0.83875`
- Final Model Recall: `0.67775`
- Final Model False Positives per Hour: `0.0`

The ONNX export path printed was:

- `sparky_acav_artifacts/wake_up_sparky_acav.onnx`

## Derived classification metrics on the synthetic test clips (threshold = 0.5)

Computed on:

- positives: `sparky_acav_artifacts/wake_up_sparky_acav/positive_features_test.npy` (8000 items)
- negatives: `sparky_acav_artifacts/wake_up_sparky_acav/negative_features_test.npy` (8000 items)

At threshold `0.5`:

- TP = `5422`
- FN = `2578`
- FP = `2`
- TN = `7998`

Derived:

- accuracy = `0.83875`
- recall = `0.67775`
- precision = `0.999631`

Interpretation: on this synthetic test set and at threshold 0.5, precision is extremely high (very few negative test clips score above 0.5), but recall is moderate (some synthetic positives are hard/mismatched and score < 0.5).

## FP/hour curve on background validation stream (HF `validation_set_features.npy`)

Background validation features:

- `data/validation_set_features.npy` (shape `(481345, 96)`)
- Evaluated as a sliding window of `win=16` frames (so `481330` windows)
- The evaluation script treats each window ≥ threshold as a “false positive window” and divides by `val_hours=11.3` to estimate FP/hour.

Using `scripts/eval_fp_curve.py` (with `--target-fp-per-hour 0.01`), results for common thresholds:

- Thresholds `0.10..0.99`: FP/hour = `0.0`
- Suggested threshold for FP/hour ≤ `0.01`: `0.042555`
- Maximum background score observed: `0.042555`

Additional spot-checks (background FP/hour):

- threshold `0.01`: FP/hour `~0.176991`
- threshold `0.03`: FP/hour `~0.088496`
- threshold `0.05`: FP/hour `0.0`

## Threshold tradeoff (synthetic positives vs background FP/hour)

Using the synthetic positive test set scores and the background validation stream:

- threshold `0.50`: synthetic recall `0.6778`, background FP/hour `0.0`
- threshold `0.05`: synthetic recall `0.7425`, background FP/hour `0.0` (on this background set)
- threshold `0.03`: synthetic recall `0.7564`, background FP/hour `~0.0885`

Practical note: it’s common to deploy with a threshold chosen to meet a stringent FP/hour target, then improve recall by adding streaming trigger logic (smoothing, debounce, refractory period) and better in-domain augmentation/training.

## How to reproduce the FP/hour sweep

Activate the isolated training environment and run:

```bash
source .venv-oww-py310/bin/activate
python scripts/eval_fp_curve.py \
  --model-onnx sparky_acav_artifacts/wake_up_sparky_acav.onnx \
  --validation-features data/validation_set_features.npy \
  --val-hours 11.3 \
  --target-fp-per-hour 0.01
```

## Operational cautions (deployment)

- The background FP/hour evaluation above is on a generic background feature stream; **real-room audio** can differ materially.
- In a continuously running system, apply a **streaming trigger policy** (e.g., require N consecutive windows above threshold, or use peak-hold + refractory period) and validate on hours of real ambient audio from the target microphone and robot hardware.

